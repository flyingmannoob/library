
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import random
from tqdm import tqdm

# 10 ä¸ªåˆ†ç±»
categories = {
    "å°è¯´": "å°è¯´",
    "å†å²": "å†å²",
    "å¿ƒç†å­¦": "å¿ƒç†å­¦",
    "è®¡ç®—æœº": "è®¡ç®—æœº",
    "æ–‡å­¦": "æ–‡å­¦",
    "ä¼ è®°": "ä¼ è®°",
    "è‰ºæœ¯": "è‰ºæœ¯",
    "ç»æµå­¦": "ç»æµå­¦",
    "æ”¿æ²»": "æ”¿æ²»",
    "ç§‘å­¦": "ç§‘å­¦"
}

# æ¯ä¸ªåˆ†ç±»æŠ“å–æ•°é‡
BOOKS_PER_CATEGORY = 10
all_books = []
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/139.0.0.0 Safari/537.36",
    "Referer": "https://book.douban.com/",
    "Cookie": "bid=Th-6Po-HVX8; dbcl2=\"203237096:9JOxbsqSmfs\"; ck=aZwU; ll=\"108289\"; _gid=GA1.2.892797493.1756799968; frodotk_db=\"d3adbba1783d11b07b0932e468b73510\"; _pk_ref.100001.3ac3=%5B%22%22%2C%22%22%2C1756799972%2C%22https%3A%2F%2Fm.douban.com%2F%22%5D; _pk_id.100001.3ac3=bcd622234923c0fa.1756799972.; _pk_ses.100001.3ac3=1; __utma=30149280.1100089963.1756799967.1756799972.1756799972.1; __utmc=30149280; __utmz=30149280.1756799972.1.1.utmcsr=m.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utma=81379588.1100089963.1756799967.1756799972.1756799972.1; __utmc=81379588; __utmz=81379588.1756799972.1.1.utmcsr=m.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; _vwo_uuid_v2=D4FBFD2240CAABFBA769736CAF2366089|94ac6933f60586ef51f720f606617ec1; __yadk_uid=XINf9wz8hrjZlRDIYsYIuferKv9BqtaG; push_noty_num=0; push_doumail_num=0; _ga=GA1.2.1100089963.1756799967; _ga_Y4GN1R87RG=GS2.1.s1756799966$o1$g1$t1756800634$j52$l0$h0; __utmt_douban=1; __utmt=1; __utmb=30149280.13.10.1756799972; __utmb=81379588.13.10.1756799972"
}

seen_books = set()  # å…¨å±€å»é‡

def fetch_url(url, retries=3):
    for i in range(retries):
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                return resp
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸ï¼š{e}, é‡è¯• {i+1}/{retries}")
        time.sleep(2)
    return None

def get_full_description(book_url):
    """æŠ“å–ä¹¦ç±è¯¦æƒ…é¡µå®Œæ•´ç®€ä»‹ï¼Œå»é™¤é‡å¤æ®µè½"""
    resp = fetch_url(book_url)
    if resp is None:
        return "æ— "
    soup = BeautifulSoup(resp.text, "html.parser")

    # ä¼˜å…ˆæŠ“æŠ˜å åå®Œæ•´å†…å®¹
    intro = soup.select(".intro.all.hidden")
    if not intro:
        # æ²¡æœ‰æŠ˜å å†…å®¹ï¼Œåˆ™æŠ“æŠ˜å å‰ç®€ä»‹
        intro = soup.select(".intro")
    if not intro:
        return "æ— "

    texts = []
    for section in intro:
        for p in section.find_all("p"):
            text = p.get_text(strip=True)
            if text:
                texts.append(text)
        if not section.find_all("p"):
            text = section.get_text(strip=True)
            if text:
                texts.append(text)

    # å»æ‰é‡å¤æ®µè½
    seen_paragraphs = set()
    final_texts = []
    for t in texts:
        if t not in seen_paragraphs:
            final_texts.append(t)
            seen_paragraphs.add(t)

    return "\n".join(final_texts)

for cname, ctag in categories.items():
    print(f"\næ­£åœ¨æŠ“å–ç±»åˆ«ï¼š{cname}")
    books = []
    start = 0

    with tqdm(total=BOOKS_PER_CATEGORY, desc=f"{cname}", ncols=100) as pbar:
        while len(books) < BOOKS_PER_CATEGORY:
            url = f"https://book.douban.com/tag/{ctag}?start={start}&type=T"
            resp = fetch_url(url)
            if resp is None:
                break

            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select("li.subject-item")
            if not items:
                break  # ç¿»åˆ°æœ€åä¸€é¡µ

            for item in items:
                title_tag = item.select_one("h2 a")
                title = title_tag["title"].strip() if title_tag else "æœªçŸ¥"
                link = title_tag["href"] if title_tag else None

                pub = item.select_one(".pub")
                author = "æœªçŸ¥"
                if pub:
                    parts = pub.text.strip().split("/")
                    if parts:
                        author = parts[0].strip()

                book_id = f"{title}_{author}"
                if book_id in seen_books:
                    continue  # å»é‡

                seen_books.add(book_id)

                description = get_full_description(link) if link else "æ— "

                books.append({
                    "åˆ†ç±»": cname,
                    "ä¹¦å": title,
                    "ä½œè€…": author,
                    "ç®€ä»‹": description
                })
                pbar.update(1)

                if len(books) >= BOOKS_PER_CATEGORY:
                    break

            start += 20
            time.sleep(random.uniform(1.5, 3.0))  # éšæœºä¼‘çœ 

    print(f"âœ… {cname} æŠ“å–å®Œæˆï¼Œå…± {len(books)} æœ¬ï¼ˆå»é‡åï¼‰")
    all_books.extend(books)

# ä¿å­˜ CSV
df = pd.DataFrame(all_books)
df.to_csv("douban_books.csv", index=False, encoding="utf-8-sig")
print("\nğŸ‰ æ‰€æœ‰åˆ†ç±»çˆ¬å–å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° douban_books.csv")

